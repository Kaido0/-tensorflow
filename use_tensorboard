# -*- coding:utf8 -*-
import tensorflow as tf
import numpy as np

#输入数据
x_data = np.linspace(-1,1,300)[:, np.newaxis]
noise = np.random.normal(0,0.05, x_data.shape)
y_data = np.square(x_data)-0.5+noise
#输入层
with tf.name_scope('input_layer'): #输入层。将这两个变量放到input_layer作用域下，tensorboard会把他们放在一个图形里面
    xs = tf.placeholder(tf.float32, [None, 1], name = 'x_input') # xs起名x_input，会在图形上显示
    ys = tf.placeholder(tf.float32, [None, 1], name = 'y_input') # ys起名y_input，会在图形上显示

#隐层
with tf.name_scope('hidden_layer'): #隐层。将隐层权重、偏置、净输入放在一起
    with tf.name_scope('weight'): #权重
        W1 = tf.Variable(tf.random_normal([1,10]),name='W1')
        tf.summary.histogram('hidden_layer/weight', W1)
    with tf.name_scope('bias'): #偏置
        b1 = tf.Variable(tf.zeros([1,10])+0.1,name='b1')
        tf.summary.histogram('hidden_layer/bias', b1)
    with tf.name_scope('Wx_plus_b'): #净输入
        Wx_plus_b1 = tf.matmul(xs,W1) + b1
        tf.summary.histogram('hidden_layer/Wx_plus_b',Wx_plus_b1)
        output1 = tf.nn.relu(Wx_plus_b1)

#输出层
with tf.name_scope('output_layer'): #输出层。将输出层权重、偏置、净输入放在一起
    with tf.name_scope('weight'): #权重
        W2 = tf.Variable(tf.random_normal([10,1]),name='W2')
        tf.summary.histogram('output_layer/weight', W2)
    with tf.name_scope('bias'): #偏置
        b2 = tf.Variable(tf.zeros([1,1])+0.1,name='b2')
        tf.summary.histogram('output_layer/bias', b2)
    with tf.name_scope('Wx_plus_b'): #净输入
        Wx_plus_b2 = tf.add(tf.matmul(output1,W2),b2)
        tf.summary.histogram('output_layer/Wx_plus_b',Wx_plus_b2)
        output2 = Wx_plus_b2

#损失
with tf.name_scope('loss'): #损失
    loss = tf.reduce_mean(tf.reduce_sum(tf.square(ys-output2),reduction_indices=[1]))
    tf.summary.scalar('loss',loss)
with tf.name_scope('train'): #训练过程
    train_step = tf.train.GradientDescentOptimizer(0.1).minimize(loss)

#初始化
init = tf.global_variables_initializer()
sess = tf.Session()
sess.run(init)
merged = tf.summary.merge_all() #将图形、训练过程等数据合并在一起
writer = tf.summary.FileWriter('logs/',sess.graph) #将训练日志写入到logs文件夹下
#将上面‘绘画’出的图保存到一个目录中，以方便后期在浏览器中可以浏览

#训练
for i in range(1000):
    sess.run(train_step,feed_dict={xs:x_data,ys:y_data})
    if(i%50==0): #每50次写一次日志
        result = sess.run(merged,feed_dict={xs:x_data,ys:y_data}) #计算需要写入的日志数据
        writer.add_summary(result,i) #将日志数据写入文件

###############################################################################################################################
###############################################################################################################################

import tensorflow as tf
from tensorflow.examples.tutorials.mnist import input_data

mnist = input_data.read_data_sets("MNIST_data/", one_hot=True)
# 输入
with tf.name_scope('input_layer'):
    x = tf.placeholder(tf.float32, [None, 784],name='x_input')  # 一行一幅图像
    y_ = tf.placeholder(tf.float32, [None, 10],name='y_input')  # 输入占位符（这张手写数字具体代表的值，0-9对应矩阵的10个位置）

with tf.name_scope('output_layer'):
    # 计算分类softmax会将xW+b分成10类，对应0-9
    with tf.name_scope('weight'):  # 权重
        W = tf.Variable(tf.zeros([784, 10]),name='W')
        tf.summary.histogram('output_layer/weight', W)
    with tf.name_scope('bias'):  # 偏置
        b = tf.Variable(tf.zeros([10]),name='b')
        tf.summary.histogram('output_layer/bias', b)
    with tf.name_scope('Wx_plus_b'):  # 净输入
        Wx_plus_b = tf.matmul(x, W) + b
        tf.summary.histogram('output_layer/Wx_plus_b', Wx_plus_b)
y = tf.nn.softmax(Wx_plus_b)

# 计算偏差和
with tf.name_scope('cross_entropy'):  # 损失
    cross_entropy = -tf.reduce_sum(y_ * tf.log(y), name='cross_entropy')
    tf.summary.scalar('cross_entropy', cross_entropy)

with tf.name_scope('loss'):
    loss = tf.reduce_mean(cross_entropy, name='loss')
    tf.summary.scalar('loss',loss)
# 使用梯度下降法（步长0.01），来使偏差和最小
with tf.name_scope('train'):  # 训练过程
    train_step = tf.train.GradientDescentOptimizer(0.01).minimize(cross_entropy)

init = tf.global_variables_initializer()
sess = tf.Session()
sess.run(init)
merged = tf.summary.merge_all()  # 将图形、训练过程等数据合并在一起
train_writer = tf.summary.FileWriter('train', sess.graph)
test_writer=tf.summary.FileWriter('test')

for i in range(10):
    batch_xs, batch_ys = mnist.train.next_batch(100)  # 随机取100个手写数字图片
    res,_ = sess.run([merged,train_step], feed_dict={x: batch_xs, y_: batch_ys})
    # res = sess.run(merged, feed_dict={x: batch_xs, y_: batch_ys})
    train_writer.add_summary(res, i)  # 将日志数据写入文件
    print sess.run(loss,feed_dict={x: batch_xs, y_: batch_ys})

# 计算训练精度
# http://stackoverflow.com/questions/41708572/tensorflow-questions-regarding-tf-argmax-and-tf-equal
# argmax返回某一个轴上最大值的索引 1：行 0：列
# tf.argmax(y,1)y是 none行10列 ，所以返回的是这张图像被识别的数字值
# tf.equal(tf.argmax(pred, 1),tf.argmax(y, 1))返回评估将给出的张量array(1,1,1,1,1,1)
correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))
# http://blog.csdn.net/lenbow/article/details/52152766
# tf.cast(x, dtype, name=None)	将x或者x.values转换为dtype
accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))

acc=sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels})
# test_writer.add_summary()
tf.summary.scalar('acc',acc)
sess.close()

###############################################################################################################################
###############################################################################################################################
